{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.5417\n",
      "The images are similar.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Define image transformations (resize, normalize, etc.)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # Resize to the input size required by ResNet\n",
    "    transforms.ToTensor(),           # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as required by ResNet\n",
    "])\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = nn.Sequential(*list(model.children())[:-1])  # Remove the final classification layer\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to extract features from an image\n",
    "def extract_features(image_path, model):\n",
    "    image = Image.open(image_path).convert('RGB')  # Load the image\n",
    "    image = transform(image).unsqueeze(0)  # Apply transformations and add a batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = model(image)  # Extract features\n",
    "    return features.squeeze().numpy()  # Return feature vector as a numpy array\n",
    "\n",
    "# Function to compare two images\n",
    "def compare_images(img1_path, img2_path, model):\n",
    "    # Extract features for both images\n",
    "    features_img1 = extract_features(img1_path, model)\n",
    "    features_img2 = extract_features(img2_path, model)\n",
    "\n",
    "    # Calculate cosine similarity between the two feature vectors\n",
    "    similarity = 1 - cosine(features_img1, features_img2)  # 1 - cosine gives similarity\n",
    "    return similarity\n",
    "\n",
    "# Paths to two images\n",
    "image1 = \"face_1.jpeg\"\n",
    "image2 = \"9.jpeg\"\n",
    "\n",
    "# Compare the images\n",
    "similarity = compare_images(image1, image2, model)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")\n",
    "\n",
    "if similarity > 0.5:  # Threshold for similarity\n",
    "    print(\"The images are similar.\")\n",
    "else:\n",
    "    print(\"The images are not similar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keshav\\anaconda3\\envs\\cuda_test\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the folder containing images\n",
    "image_folder = 'image_dataset'\n",
    "\n",
    "# Get a list of all image files in the folder\n",
    "image_files = glob(os.path.join(image_folder, '*.jpg'))\n",
    "\n",
    "# Ensure the sample size does not exceed the number of available images\n",
    "sample_size = min(10, len(image_files))\n",
    "\n",
    "# Randomly select images\n",
    "random.seed(42)\n",
    "selected_images = random.sample(image_files, sample_size)\n",
    "\n",
    "# Display the selected images\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i, image_path in enumerate(selected_images):\n",
    "    img = Image.open(image_path)\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clip_embeddings(images_path, model):\n",
    "\n",
    "    image_paths = glob(os.path.join(images_path, '**/*.jpg'), recursive=True)\n",
    "\n",
    "    embeddings = []\n",
    "    for img_path in image_paths:\n",
    "        image = Image.open(img_path)\n",
    "        embedding = model.encode(image)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    return embeddings, image_paths\n",
    "\n",
    "\n",
    "\n",
    "IMAGES_PATH = 'E:/STGI/Dataset/test/FAKE'\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "# embeddings, image_paths = generate_clip_embeddings(IMAGES_PATH, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the IndexIVFPQ with 10000 vectors...\n",
      "Training completed.\n",
      "Adding vectors to the index...\n",
      "Total vectors indexed: 10000\n",
      "Index created and saved to vector.index\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def create_faiss_index_ivfpq(embeddings, image_paths, output_path, \n",
    "                             nlist=1000, m=16, nbits=8, \n",
    "                             training_size=100000):\n",
    "    \"\"\"\n",
    "    Creates a FAISS IndexIVFPQ index for efficient similarity search.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings (List[List[float]]): List of image feature vectors.\n",
    "    - image_paths (List[str]): Corresponding image file paths.\n",
    "    - output_path (str): Path to save the FAISS index.\n",
    "    - nlist (int): Number of clusters for IVF.\n",
    "    - m (int): Number of PQ subquantizers.\n",
    "    - nbits (int): Number of bits per subquantizer.\n",
    "    - training_size (int): Number of vectors to use for training.\n",
    "\n",
    "    Returns:\n",
    "    - index (faiss.Index): Trained FAISS index.\n",
    "    \"\"\"\n",
    "    # Convert embeddings to NumPy array\n",
    "    vectors = np.array(embeddings).astype('float32')\n",
    "    dimension = vectors.shape[1]\n",
    "\n",
    "    # Define the quantizer (IndexFlatIP for inner product)\n",
    "    quantizer = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "    # Initialize the IndexIVFPQ\n",
    "    index_ivfpq = faiss.IndexIVFPQ(quantizer, dimension, nlist, m, nbits)\n",
    "\n",
    "    # Wrap with IndexIDMap to handle custom IDs\n",
    "    index = faiss.IndexIDMap(index_ivfpq)\n",
    "\n",
    "    # Training the index\n",
    "    if not index_ivfpq.is_trained:\n",
    "        # Select a random subset for training\n",
    "        np.random.seed(123)  # For reproducibility\n",
    "        if training_size > len(vectors):\n",
    "            training_size = len(vectors)\n",
    "        random_indices = np.random.choice(len(vectors), size=training_size, replace=False)\n",
    "        training_vectors = vectors[random_indices]\n",
    "        print(f\"Training the IndexIVFPQ with {training_size} vectors...\")\n",
    "        index_ivfpq.train(training_vectors)\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    # Assign unique IDs to each vector\n",
    "    ids = np.arange(len(embeddings)).astype('int64')\n",
    "\n",
    "    # Add vectors with their IDs to the index\n",
    "    print(\"Adding vectors to the index...\")\n",
    "    index.add_with_ids(vectors, ids)\n",
    "    print(f\"Total vectors indexed: {index.ntotal}\")\n",
    "\n",
    "    # Save the index\n",
    "    faiss.write_index(index, output_path)\n",
    "    print(f\"Index created and saved to {output_path}\")\n",
    "\n",
    "    # Save image paths with their corresponding IDs\n",
    "    with open(output_path + '.paths', 'w') as f:\n",
    "        for img_path in image_paths:\n",
    "            f.write(img_path + '\\n')\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "OUTPUT_INDEX_PATH = \"vector.index\"\n",
    "index = create_faiss_index_ivfpq(embeddings, image_paths, OUTPUT_INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded from vector.index\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_INDEX_PATH = \"vector.index\"\n",
    "import faiss\n",
    "\n",
    "def load_faiss_index(index_path):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(index_path + '.paths', 'r') as f:\n",
    "        image_paths = [line.strip() for line in f]\n",
    "    print(f\"Index loaded from {index_path}\")\n",
    "    return index, image_paths\n",
    "\n",
    "index, image_paths = load_faiss_index(OUTPUT_INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_images(query, model, index, image_paths, top_k=3):\n",
    "\n",
    "    if query.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "        query = Image.open(query)\n",
    "\n",
    "    query_features = model.encode(query)\n",
    "    query_features = query_features.astype(np.float32).reshape(1, -1)\n",
    "\n",
    "    distances, indices = index.search(query_features, top_k)\n",
    "\n",
    "    retrieved_images = [image_paths[int(idx)] for idx in indices[0]]\n",
    "\n",
    "    return query, retrieved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(query, retrieved_images):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # If image query\n",
    "    if isinstance(query, Image.Image):\n",
    "        plt.subplot(1, len(retrieved_images) + 1, 1)\n",
    "        plt.imshow(query)\n",
    "        plt.title(\"Query Image\")\n",
    "        plt.axis('off')\n",
    "        start_idx = 2\n",
    "\n",
    "    # If text query\n",
    "    else:\n",
    "        plt.subplot(1, len(retrieved_images) + 1, 1)\n",
    "        plt.text(0.5, 0.5, f\"Query:\\n\\n '{query}'\", fontsize=16, ha='center', va='center')\n",
    "        plt.axis('off')\n",
    "        start_idx = 2\n",
    "\n",
    "    # Display images\n",
    "    for i, img_path in enumerate(retrieved_images):\n",
    "\n",
    "        plt.subplot(1, len(retrieved_images) + 1, i + start_idx)\n",
    "        plt.imshow(Image.open(img_path))\n",
    "        plt.title(f\"Match {i + 1}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "query = 'image_dataset\\image_dataset\\pexels-andrew-3201768.jpg'\n",
    "query, retrieved_images = retrieve_similar_images(query, model, index, image_paths, top_k=3)\n",
    "visualize_results(query, retrieved_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
